[MAdd]: AdaptiveAvgPool2d is not supported!
[Flops]: AdaptiveAvgPool2d is not supported!
[Memory]: AdaptiveAvgPool2d is not supported!
                 module name   input shape  output shape      params memory(MB)             MAdd            Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)    params             MAdd
0                      conv1     3 224 224    64 112 112      9408.0       3.06    235,225,088.0    118,013,952.0    639744.0    3211264.0       3.64%    3851008.0    9408 = 7*7*3*64    235,225,088.0 = 2*7*7*3*64*112*112
1                        bn1    64 112 112    64 112 112       128.0       3.06      3,211,264.0      1,605,632.0   3211776.0    3211264.0       0.61%    6423040.0    128  = 64*2        3,211,264.0 = 4*64*112*112
2                       relu    64 112 112    64 112 112         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.20%    6422528.0                       802,816.0 = 64*112*112
3                    maxpool    64 112 112    64  56  56         0.0       0.77      1,605,632.0        802,816.0   3211264.0     802816.0       2.82%    4014080.0                       1,605,632.0 = 2*64*112*112
4             layer1.0.conv1    64  56  56    64  56  56      4096.0       0.77     25,489,408.0     12,845,056.0    819200.0     802816.0       1.38%    1622016.0    4096 = 1*164*64    25,489,408.0 = 2*1*1*64*64*56*56
5               layer1.0.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.21%    1606144.0    128 = 64*2         802,816.0 = 4*64*56*56
6             layer1.0.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       2.51%    1753088.0    36864 = 3*3*64*64  231,010,304.0 = 2*3*3*64*64*56*56
7               layer1.0.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.18%    1606144.0    128 = 64*2         802,816.0 = 4*64*56*56
8             layer1.0.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       2.05%    4079616.0    16384 = 1*1*64*256 101,957,632.0 = 2*1*1*64*256*64*64
9               layer1.0.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.51%    6424576.0    512 = 256*2        3,211,264.0 = 4*256*56*56
10             layer1.0.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.12%    6422528.0                       802,816.0 = 256*56*56
11     layer1.0.downsample.0    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.47%    4079616.0    16384 = 1*1*64*256
12     layer1.0.downsample.1   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.53%    6424576.0    512 = 256*2
13            layer1.1.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.57%    4079616.0    16384 = 1*1*256*64
14              layer1.1.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.11%    1606144.0    128 = 64*2
15            layer1.1.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.55%    1753088.0    36864 = 3*3*64*64
16              layer1.1.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.08%    1606144.0    128 = 64*2
17            layer1.1.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.27%    4079616.0    16384 = 1*1*64*64
18              layer1.1.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.52%    6424576.0    512 = 256*2
19             layer1.1.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.36%    6422528.0
20            layer1.2.conv1   256  56  56    64  56  56     16384.0       0.77    102,559,744.0     51,380,224.0   3276800.0     802816.0       1.41%    4079616.0
21              layer1.2.bn1    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.13%    1606144.0
22            layer1.2.conv2    64  56  56    64  56  56     36864.0       0.77    231,010,304.0    115,605,504.0    950272.0     802816.0       1.55%    1753088.0
23              layer1.2.bn2    64  56  56    64  56  56       128.0       0.77        802,816.0        401,408.0    803328.0     802816.0       0.11%    1606144.0
24            layer1.2.conv3    64  56  56   256  56  56     16384.0       3.06    101,957,632.0     51,380,224.0    868352.0    3211264.0       1.73%    4079616.0
25              layer1.2.bn3   256  56  56   256  56  56       512.0       3.06      3,211,264.0      1,605,632.0   3213312.0    3211264.0       0.61%    6424576.0
26             layer1.2.relu   256  56  56   256  56  56         0.0       3.06        802,816.0        802,816.0   3211264.0    3211264.0       0.12%    6422528.0
27            layer2.0.conv1   256  56  56   128  56  56     32768.0       1.53    205,119,488.0    102,760,448.0   3342336.0    1605632.0       2.64%    4947968.0
28              layer2.0.bn1   128  56  56   128  56  56       256.0       1.53      1,605,632.0        802,816.0   1606656.0    1605632.0       0.14%    3212288.0
29            layer2.0.conv2   128  56  56   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0   2195456.0     401408.0       2.33%    2596864.0
30              layer2.0.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.08%     803840.0
31            layer2.0.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.52%    2269184.0
32              layer2.0.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.31%    3215360.0
33             layer2.0.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.07%    3211264.0
34     layer2.0.downsample.0   256  56  56   512  28  28    131072.0       1.53    205,119,488.0    102,760,448.0   3735552.0    1605632.0       2.84%    5341184.0
35     layer2.0.downsample.1   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.22%    3215360.0
36            layer2.1.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.37%    2269184.0
37              layer2.1.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.08%     803840.0
38            layer2.1.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       1.85%    1392640.0
39              layer2.1.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.07%     803840.0
40            layer2.1.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       0.86%    2269184.0
41              layer2.1.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.15%    3215360.0
42             layer2.1.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.04%    3211264.0
43            layer2.2.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.12%    2269184.0
44              layer2.2.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
45            layer2.2.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       1.48%    1392640.0
46              layer2.2.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.06%     803840.0
47            layer2.2.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.02%    2269184.0
48              layer2.2.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.33%    3215360.0
49             layer2.2.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.06%    3211264.0
50            layer2.3.conv1   512  28  28   128  28  28     65536.0       0.38    102,660,096.0     51,380,224.0   1867776.0     401408.0       1.10%    2269184.0
51              layer2.3.bn1   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.09%     803840.0
52            layer2.3.conv2   128  28  28   128  28  28    147456.0       0.38    231,110,656.0    115,605,504.0    991232.0     401408.0       1.64%    1392640.0
53              layer2.3.bn2   128  28  28   128  28  28       256.0       0.38        401,408.0        200,704.0    402432.0     401408.0       0.07%     803840.0
54            layer2.3.conv3   128  28  28   512  28  28     65536.0       1.53    102,359,040.0     51,380,224.0    663552.0    1605632.0       1.00%    2269184.0
55              layer2.3.bn3   512  28  28   512  28  28      1024.0       1.53      1,605,632.0        802,816.0   1609728.0    1605632.0       0.30%    3215360.0
56             layer2.3.relu   512  28  28   512  28  28         0.0       1.53        401,408.0        401,408.0   1605632.0    1605632.0       0.06%    3211264.0
57            layer3.0.conv1   512  28  28   256  28  28    131072.0       0.77    205,320,192.0    102,760,448.0   2129920.0     802816.0       2.23%    2932736.0
58              layer3.0.bn1   256  28  28   256  28  28       512.0       0.77        802,816.0        401,408.0    804864.0     802816.0       0.11%    1607680.0
59            layer3.0.conv2   256  28  28   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   3162112.0     200704.0       2.23%    3362816.0
60              layer3.0.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.07%     403456.0
61            layer3.0.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       1.43%    2052096.0
62              layer3.0.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.10%    1613824.0
63             layer3.0.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.04%    1605632.0
64     layer3.0.downsample.0   512  28  28  1024  14  14    524288.0       0.77    205,320,192.0    102,760,448.0   3702784.0     802816.0       2.54%    4505600.0
65     layer3.0.downsample.1  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.10%    1613824.0
66            layer3.1.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.35%    2052096.0
67              layer3.1.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
68            layer3.1.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.74%    2760704.0
69              layer3.1.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
70            layer3.1.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.81%    2052096.0
71              layer3.1.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0
72             layer3.1.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
73            layer3.2.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.85%    2052096.0
74              layer3.2.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
75            layer3.2.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.51%    2760704.0
76              layer3.2.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.05%     403456.0
77            layer3.2.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.82%    2052096.0
78              layer3.2.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.08%    1613824.0
79             layer3.2.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
80            layer3.3.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       0.99%    2052096.0
81              layer3.3.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
82            layer3.3.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.68%    2760704.0
83              layer3.3.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
84            layer3.3.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.83%    2052096.0
85              layer3.3.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.08%    1613824.0
86             layer3.3.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
87            layer3.4.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.09%    2052096.0
88              layer3.4.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
89            layer3.4.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.65%    2760704.0
90              layer3.4.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.05%     403456.0
91            layer3.4.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.81%    2052096.0
92              layer3.4.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0
93             layer3.4.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
94            layer3.5.conv1  1024  14  14   256  14  14    262144.0       0.19    102,710,272.0     51,380,224.0   1851392.0     200704.0       1.05%    2052096.0
95              layer3.5.bn1   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.05%     403456.0
96            layer3.5.conv2   256  14  14   256  14  14    589824.0       0.19    231,160,832.0    115,605,504.0   2560000.0     200704.0       1.63%    2760704.0
97              layer3.5.bn2   256  14  14   256  14  14       512.0       0.19        200,704.0        100,352.0    202752.0     200704.0       0.06%     403456.0
98            layer3.5.conv3   256  14  14  1024  14  14    262144.0       0.77    102,559,744.0     51,380,224.0   1249280.0     802816.0       0.81%    2052096.0
99              layer3.5.bn3  1024  14  14  1024  14  14      2048.0       0.77        802,816.0        401,408.0    811008.0     802816.0       0.07%    1613824.0
100            layer3.5.relu  1024  14  14  1024  14  14         0.0       0.77        200,704.0        200,704.0    802816.0     802816.0       0.03%    1605632.0
101           layer4.0.conv1  1024  14  14   512  14  14    524288.0       0.38    205,420,544.0    102,760,448.0   2899968.0     401408.0       2.38%    3301376.0
102             layer4.0.bn1   512  14  14   512  14  14      1024.0       0.38        401,408.0        200,704.0    405504.0     401408.0       0.09%     806912.0
103           layer4.0.conv2   512  14  14   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9838592.0     100352.0       4.18%    9938944.0
104             layer4.0.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.09%     204800.0
105           layer4.0.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       2.14%    4696064.0
106             layer4.0.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.11%     819200.0
107            layer4.0.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0
108    layer4.0.downsample.0  1024  14  14  2048   7   7   2097152.0       0.38    205,420,544.0    102,760,448.0   9191424.0     401408.0       3.16%    9592832.0
109    layer4.0.downsample.1  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.13%     819200.0
110           layer4.1.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       1.73%    4696064.0
111             layer4.1.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.08%     204800.0
112           layer4.1.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       2.94%    9637888.0
113             layer4.1.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.08%     204800.0
114           layer4.1.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       0.91%    4696064.0
115             layer4.1.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.09%     819200.0
116            layer4.1.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.03%     802816.0
117           layer4.2.conv1  2048   7   7   512   7   7   1048576.0       0.10    102,735,360.0     51,380,224.0   4595712.0     100352.0       0.75%    4696064.0
118             layer4.2.bn1   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.06%     204800.0
119           layer4.2.conv2   512   7   7   512   7   7   2359296.0       0.10    231,185,920.0    115,605,504.0   9537536.0     100352.0       1.54%    9637888.0
120             layer4.2.bn2   512   7   7   512   7   7      1024.0       0.10        100,352.0         50,176.0    104448.0     100352.0       0.07%     204800.0
121           layer4.2.conv3   512   7   7  2048   7   7   1048576.0       0.38    102,660,096.0     51,380,224.0   4294656.0     401408.0       0.67%    4696064.0
122             layer4.2.bn3  2048   7   7  2048   7   7      4096.0       0.38        401,408.0        200,704.0    417792.0     401408.0       0.08%     819200.0
123            layer4.2.relu  2048   7   7  2048   7   7         0.0       0.38        100,352.0        100,352.0    401408.0     401408.0       0.04%     802816.0
124                  avgpool  2048   7   7  2048   1   1         0.0       0.01              0.0              0.0         0.0          0.0       0.21%          0.0
125                       fc          2048          1000   2049000.0       0.00      4,095,000.0      2,048,000.0   8204192.0       4000.0       0.46%    8208192.0
total                                                     25557032.0     109.69  8,219,637,272.0  4,118,537,216.0   8204192.0       4000.0     100.00%  332849216.0
===================================================================================================================================================================
Total params: 25,557,032
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total memory: 109.69MB
Total MAdd: 8.22GMAdd
Total Flops: 4.12GFlops
Total MemR+W: 317.43MB

Warning: module Bottleneck is treated as a zero-op.
Warning: module ResNet is treated as a zero-op.
[MAdd]: AdaptiveAvgPool2d is not supported!
[Flops]: AdaptiveAvgPool2d is not supported!
[Memory]: AdaptiveAvgPool2d is not supported!
ResNet(
  25.557 M, 100.000% Params, 4.122 GMac, 100.000% MACs, 
  (conv1): Conv2d(0.009 M, 0.037% Params, 0.118 GMac, 2.863% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.002 GMac, 0.039% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.019% MACs, inplace=True)
  (maxpool): MaxPool2d(0.0 M, 0.000% Params, 0.001 GMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    0.216 M, 0.844% Params, 0.68 GMac, 16.507% MACs, 
    (0): Bottleneck(
      0.075 M, 0.293% Params, 0.236 GMac, 5.736% MACs, 
      (conv1): Conv2d(0.004 M, 0.016% Params, 0.013 GMac, 0.312% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.037 M, 0.144% Params, 0.116 GMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.016 M, 0.064% Params, 0.051 GMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.029% MACs, inplace=True)
      (downsample): Sequential(
        0.017 M, 0.066% Params, 0.053 GMac, 1.285% MACs, 
        (0): Conv2d(0.016 M, 0.064% Params, 0.051 GMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      0.07 M, 0.275% Params, 0.222 GMac, 5.385% MACs, 
      (conv1): Conv2d(0.016 M, 0.064% Params, 0.051 GMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.037 M, 0.144% Params, 0.116 GMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.016 M, 0.064% Params, 0.051 GMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.029% MACs, inplace=True)
    )
    (2): Bottleneck(
      0.07 M, 0.275% Params, 0.222 GMac, 5.385% MACs, 
      (conv1): Conv2d(0.016 M, 0.064% Params, 0.051 GMac, 1.247% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.037 M, 0.144% Params, 0.116 GMac, 2.805% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.010% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.016 M, 0.064% Params, 0.051 GMac, 1.247% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.001 M, 0.002% Params, 0.002 GMac, 0.039% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.029% MACs, inplace=True)
    )
  )
  (layer2): Sequential(
    1.22 M, 4.772% Params, 1.037 GMac, 25.147% MACs, 
    (0): Bottleneck(
      0.379 M, 1.484% Params, 0.376 GMac, 9.122% MACs, 
      (conv1): Conv2d(0.033 M, 0.128% Params, 0.103 GMac, 2.493% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.001 GMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.147 M, 0.577% Params, 0.116 GMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.066 M, 0.256% Params, 0.051 GMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.022% MACs, inplace=True)
      (downsample): Sequential(
        0.132 M, 0.517% Params, 0.104 GMac, 2.512% MACs, 
        (0): Conv2d(0.131 M, 0.513% Params, 0.103 GMac, 2.493% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      0.28 M, 1.096% Params, 0.22 GMac, 5.341% MACs, 
      (conv1): Conv2d(0.066 M, 0.256% Params, 0.051 GMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.147 M, 0.577% Params, 0.116 GMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.066 M, 0.256% Params, 0.051 GMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.015% MACs, inplace=True)
    )
    (2): Bottleneck(
      0.28 M, 1.096% Params, 0.22 GMac, 5.341% MACs, 
      (conv1): Conv2d(0.066 M, 0.256% Params, 0.051 GMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.147 M, 0.577% Params, 0.116 GMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.066 M, 0.256% Params, 0.051 GMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.015% MACs, inplace=True)
    )
    (3): Bottleneck(
      0.28 M, 1.096% Params, 0.22 GMac, 5.341% MACs, 
      (conv1): Conv2d(0.066 M, 0.256% Params, 0.051 GMac, 1.247% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.147 M, 0.577% Params, 0.116 GMac, 2.805% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.0 M, 0.001% Params, 0.0 GMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.066 M, 0.256% Params, 0.051 GMac, 1.247% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.001 M, 0.004% Params, 0.001 GMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.001 GMac, 0.015% MACs, inplace=True)
    )
  )
  (layer3): Sequential(
    7.098 M, 27.775% Params, 1.471 GMac, 35.678% MACs, 
    (0): Bottleneck(
      1.512 M, 5.918% Params, 0.374 GMac, 9.080% MACs, 
      (conv1): Conv2d(0.131 M, 0.513% Params, 0.103 GMac, 2.493% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.59 M, 2.308% Params, 0.116 GMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.002 M, 0.008% Params, 0.0 GMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.011% MACs, inplace=True)
      (downsample): Sequential(
        0.526 M, 2.059% Params, 0.103 GMac, 2.503% MACs, 
        (0): Conv2d(0.524 M, 2.051% Params, 0.103 GMac, 2.493% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(0.002 M, 0.008% Params, 0.0 GMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      1.117 M, 4.371% Params, 0.219 GMac, 5.320% MACs, 
      (conv1): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.59 M, 2.308% Params, 0.116 GMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.002 M, 0.008% Params, 0.0 GMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)
    )
    (2): Bottleneck(
      1.117 M, 4.371% Params, 0.219 GMac, 5.320% MACs, 
      (conv1): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.59 M, 2.308% Params, 0.116 GMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.002 M, 0.008% Params, 0.0 GMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)
    )
    (3): Bottleneck(
      1.117 M, 4.371% Params, 0.219 GMac, 5.320% MACs, 
      (conv1): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.59 M, 2.308% Params, 0.116 GMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.002 M, 0.008% Params, 0.0 GMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)
    )
    (4): Bottleneck(
      1.117 M, 4.371% Params, 0.219 GMac, 5.320% MACs, 
      (conv1): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.59 M, 2.308% Params, 0.116 GMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.002 M, 0.008% Params, 0.0 GMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)
    )
    (5): Bottleneck(
      1.117 M, 4.371% Params, 0.219 GMac, 5.320% MACs, 
      (conv1): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(0.59 M, 2.308% Params, 0.116 GMac, 2.805% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.002% Params, 0.0 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(0.262 M, 1.026% Params, 0.051 GMac, 1.247% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.002 M, 0.008% Params, 0.0 GMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.007% MACs, inplace=True)
    )
  )
  (layer4): Sequential(
    14.965 M, 58.554% Params, 0.811 GMac, 19.676% MACs, 
    (0): Bottleneck(
      6.04 M, 23.632% Params, 0.373 GMac, 9.059% MACs, 
      (conv1): Conv2d(0.524 M, 2.051% Params, 0.103 GMac, 2.493% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.359 M, 9.231% Params, 0.116 GMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.049 M, 4.103% Params, 0.051 GMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.004 M, 0.016% Params, 0.0 GMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.005% MACs, inplace=True)
      (downsample): Sequential(
        2.101 M, 8.222% Params, 0.103 GMac, 2.498% MACs, 
        (0): Conv2d(2.097 M, 8.206% Params, 0.103 GMac, 2.493% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(0.004 M, 0.016% Params, 0.0 GMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      4.463 M, 17.461% Params, 0.219 GMac, 5.309% MACs, 
      (conv1): Conv2d(1.049 M, 4.103% Params, 0.051 GMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.359 M, 9.231% Params, 0.116 GMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.049 M, 4.103% Params, 0.051 GMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.004 M, 0.016% Params, 0.0 GMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs, inplace=True)
    )
    (2): Bottleneck(
      4.463 M, 17.461% Params, 0.219 GMac, 5.309% MACs, 
      (conv1): Conv2d(1.049 M, 4.103% Params, 0.051 GMac, 1.247% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(2.359 M, 9.231% Params, 0.116 GMac, 2.805% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(0.001 M, 0.004% Params, 0.0 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(1.049 M, 4.103% Params, 0.051 GMac, 1.247% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(0.004 M, 0.016% Params, 0.0 GMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs, inplace=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(0.0 M, 0.000% Params, 0.0 GMac, 0.002% MACs, output_size=(1, 1))
  (fc): Linear(2.049 M, 8.017% Params, 0.002 GMac, 0.050% MACs, in_features=2048, out_features=1000, bias=True)
)
Computational complexity:       4.12 GMac
Number of parameters:           25.56 M 
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
[MAdd]: AdaptiveAvgPool2d is not supported!
[Flops]: AdaptiveAvgPool2d is not supported!
[Memory]: AdaptiveAvgPool2d is not supported!
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 112, 112]           9,408
       BatchNorm2d-2         [-1, 64, 112, 112]             128
              ReLU-3         [-1, 64, 112, 112]               0
         MaxPool2d-4           [-1, 64, 56, 56]               0
            Conv2d-5           [-1, 64, 56, 56]           4,096
       BatchNorm2d-6           [-1, 64, 56, 56]             128
              ReLU-7           [-1, 64, 56, 56]               0
            Conv2d-8           [-1, 64, 56, 56]          36,864
       BatchNorm2d-9           [-1, 64, 56, 56]             128
             ReLU-10           [-1, 64, 56, 56]               0
           Conv2d-11          [-1, 256, 56, 56]          16,384
      BatchNorm2d-12          [-1, 256, 56, 56]             512
           Conv2d-13          [-1, 256, 56, 56]          16,384
      BatchNorm2d-14          [-1, 256, 56, 56]             512
             ReLU-15          [-1, 256, 56, 56]               0
       Bottleneck-16          [-1, 256, 56, 56]               0
           Conv2d-17           [-1, 64, 56, 56]          16,384
      BatchNorm2d-18           [-1, 64, 56, 56]             128
             ReLU-19           [-1, 64, 56, 56]               0
           Conv2d-20           [-1, 64, 56, 56]          36,864
      BatchNorm2d-21           [-1, 64, 56, 56]             128
             ReLU-22           [-1, 64, 56, 56]               0
           Conv2d-23          [-1, 256, 56, 56]          16,384
      BatchNorm2d-24          [-1, 256, 56, 56]             512
             ReLU-25          [-1, 256, 56, 56]               0
       Bottleneck-26          [-1, 256, 56, 56]               0
           Conv2d-27           [-1, 64, 56, 56]          16,384
      BatchNorm2d-28           [-1, 64, 56, 56]             128
             ReLU-29           [-1, 64, 56, 56]               0
           Conv2d-30           [-1, 64, 56, 56]          36,864
      BatchNorm2d-31           [-1, 64, 56, 56]             128
             ReLU-32           [-1, 64, 56, 56]               0
           Conv2d-33          [-1, 256, 56, 56]          16,384
      BatchNorm2d-34          [-1, 256, 56, 56]             512
             ReLU-35          [-1, 256, 56, 56]               0
       Bottleneck-36          [-1, 256, 56, 56]               0
           Conv2d-37          [-1, 128, 56, 56]          32,768
      BatchNorm2d-38          [-1, 128, 56, 56]             256
             ReLU-39          [-1, 128, 56, 56]               0
           Conv2d-40          [-1, 128, 28, 28]         147,456
      BatchNorm2d-41          [-1, 128, 28, 28]             256
             ReLU-42          [-1, 128, 28, 28]               0
           Conv2d-43          [-1, 512, 28, 28]          65,536
      BatchNorm2d-44          [-1, 512, 28, 28]           1,024
           Conv2d-45          [-1, 512, 28, 28]         131,072
      BatchNorm2d-46          [-1, 512, 28, 28]           1,024
             ReLU-47          [-1, 512, 28, 28]               0
       Bottleneck-48          [-1, 512, 28, 28]               0
           Conv2d-49          [-1, 128, 28, 28]          65,536
      BatchNorm2d-50          [-1, 128, 28, 28]             256
             ReLU-51          [-1, 128, 28, 28]               0
           Conv2d-52          [-1, 128, 28, 28]         147,456
      BatchNorm2d-53          [-1, 128, 28, 28]             256
             ReLU-54          [-1, 128, 28, 28]               0
           Conv2d-55          [-1, 512, 28, 28]          65,536
      BatchNorm2d-56          [-1, 512, 28, 28]           1,024
             ReLU-57          [-1, 512, 28, 28]               0
       Bottleneck-58          [-1, 512, 28, 28]               0
           Conv2d-59          [-1, 128, 28, 28]          65,536
      BatchNorm2d-60          [-1, 128, 28, 28]             256
             ReLU-61          [-1, 128, 28, 28]               0
           Conv2d-62          [-1, 128, 28, 28]         147,456
      BatchNorm2d-63          [-1, 128, 28, 28]             256
             ReLU-64          [-1, 128, 28, 28]               0
           Conv2d-65          [-1, 512, 28, 28]          65,536
      BatchNorm2d-66          [-1, 512, 28, 28]           1,024
             ReLU-67          [-1, 512, 28, 28]               0
       Bottleneck-68          [-1, 512, 28, 28]               0
           Conv2d-69          [-1, 128, 28, 28]          65,536
      BatchNorm2d-70          [-1, 128, 28, 28]             256
             ReLU-71          [-1, 128, 28, 28]               0
           Conv2d-72          [-1, 128, 28, 28]         147,456
      BatchNorm2d-73          [-1, 128, 28, 28]             256
             ReLU-74          [-1, 128, 28, 28]               0
           Conv2d-75          [-1, 512, 28, 28]          65,536
      BatchNorm2d-76          [-1, 512, 28, 28]           1,024
             ReLU-77          [-1, 512, 28, 28]               0
       Bottleneck-78          [-1, 512, 28, 28]               0
           Conv2d-79          [-1, 256, 28, 28]         131,072
      BatchNorm2d-80          [-1, 256, 28, 28]             512
             ReLU-81          [-1, 256, 28, 28]               0
           Conv2d-82          [-1, 256, 14, 14]         589,824
      BatchNorm2d-83          [-1, 256, 14, 14]             512
             ReLU-84          [-1, 256, 14, 14]               0
           Conv2d-85         [-1, 1024, 14, 14]         262,144
      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048
           Conv2d-87         [-1, 1024, 14, 14]         524,288
      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048
             ReLU-89         [-1, 1024, 14, 14]               0
       Bottleneck-90         [-1, 1024, 14, 14]               0
           Conv2d-91          [-1, 256, 14, 14]         262,144
      BatchNorm2d-92          [-1, 256, 14, 14]             512
             ReLU-93          [-1, 256, 14, 14]               0
           Conv2d-94          [-1, 256, 14, 14]         589,824
      BatchNorm2d-95          [-1, 256, 14, 14]             512
             ReLU-96          [-1, 256, 14, 14]               0
           Conv2d-97         [-1, 1024, 14, 14]         262,144
      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048
             ReLU-99         [-1, 1024, 14, 14]               0
      Bottleneck-100         [-1, 1024, 14, 14]               0
          Conv2d-101          [-1, 256, 14, 14]         262,144
     BatchNorm2d-102          [-1, 256, 14, 14]             512
            ReLU-103          [-1, 256, 14, 14]               0
          Conv2d-104          [-1, 256, 14, 14]         589,824
     BatchNorm2d-105          [-1, 256, 14, 14]             512
            ReLU-106          [-1, 256, 14, 14]               0
          Conv2d-107         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048
            ReLU-109         [-1, 1024, 14, 14]               0
      Bottleneck-110         [-1, 1024, 14, 14]               0
          Conv2d-111          [-1, 256, 14, 14]         262,144
     BatchNorm2d-112          [-1, 256, 14, 14]             512
            ReLU-113          [-1, 256, 14, 14]               0
          Conv2d-114          [-1, 256, 14, 14]         589,824
     BatchNorm2d-115          [-1, 256, 14, 14]             512
            ReLU-116          [-1, 256, 14, 14]               0
          Conv2d-117         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048
            ReLU-119         [-1, 1024, 14, 14]               0
      Bottleneck-120         [-1, 1024, 14, 14]               0
          Conv2d-121          [-1, 256, 14, 14]         262,144
     BatchNorm2d-122          [-1, 256, 14, 14]             512
            ReLU-123          [-1, 256, 14, 14]               0
          Conv2d-124          [-1, 256, 14, 14]         589,824
     BatchNorm2d-125          [-1, 256, 14, 14]             512
            ReLU-126          [-1, 256, 14, 14]               0
          Conv2d-127         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048
            ReLU-129         [-1, 1024, 14, 14]               0
      Bottleneck-130         [-1, 1024, 14, 14]               0
          Conv2d-131          [-1, 256, 14, 14]         262,144
     BatchNorm2d-132          [-1, 256, 14, 14]             512
            ReLU-133          [-1, 256, 14, 14]               0
          Conv2d-134          [-1, 256, 14, 14]         589,824
     BatchNorm2d-135          [-1, 256, 14, 14]             512
            ReLU-136          [-1, 256, 14, 14]               0
          Conv2d-137         [-1, 1024, 14, 14]         262,144
     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048
            ReLU-139         [-1, 1024, 14, 14]               0
      Bottleneck-140         [-1, 1024, 14, 14]               0
          Conv2d-141          [-1, 512, 14, 14]         524,288
     BatchNorm2d-142          [-1, 512, 14, 14]           1,024
            ReLU-143          [-1, 512, 14, 14]               0
          Conv2d-144            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-145            [-1, 512, 7, 7]           1,024
            ReLU-146            [-1, 512, 7, 7]               0
          Conv2d-147           [-1, 2048, 7, 7]       1,048,576
     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096
          Conv2d-149           [-1, 2048, 7, 7]       2,097,152
     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096
            ReLU-151           [-1, 2048, 7, 7]               0
      Bottleneck-152           [-1, 2048, 7, 7]               0
          Conv2d-153            [-1, 512, 7, 7]       1,048,576
     BatchNorm2d-154            [-1, 512, 7, 7]           1,024
            ReLU-155            [-1, 512, 7, 7]               0
          Conv2d-156            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-157            [-1, 512, 7, 7]           1,024
            ReLU-158            [-1, 512, 7, 7]               0
          Conv2d-159           [-1, 2048, 7, 7]       1,048,576
     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096
            ReLU-161           [-1, 2048, 7, 7]               0
      Bottleneck-162           [-1, 2048, 7, 7]               0
          Conv2d-163            [-1, 512, 7, 7]       1,048,576
     BatchNorm2d-164            [-1, 512, 7, 7]           1,024
            ReLU-165            [-1, 512, 7, 7]               0
          Conv2d-166            [-1, 512, 7, 7]       2,359,296
     BatchNorm2d-167            [-1, 512, 7, 7]           1,024
            ReLU-168            [-1, 512, 7, 7]               0
          Conv2d-169           [-1, 2048, 7, 7]       1,048,576
     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096
            ReLU-171           [-1, 2048, 7, 7]               0
      Bottleneck-172           [-1, 2048, 7, 7]               0
AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0
          Linear-174                 [-1, 1000]       2,049,000
================================================================
Total params: 25,557,032
Trainable params: 25,557,032
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 286.56
Params size (MB): 97.49
Estimated Total Size (MB): 384.62
----------------------------------------------------------------
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Number of parameter: 25.56M
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
Total params: 25.56M
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
